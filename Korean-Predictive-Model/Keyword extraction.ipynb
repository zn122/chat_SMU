{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPzIcgF4T0MhkSVJCsW+wTO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["*감성사전 키워드 추출*\n","-> keyword(감성사전)\n","\n","*fasttext 키워드 추출*\n","-> keyword(fasttext_1_gram), keyword(fasttext_2_grams), keyword(fasttext_n_grams)\n","\n","*fasttext 인코딩 오류 제거*\n","-> keyword(fasttext_1_gram_오류제거), keyword(fasttext_2_grams_오류제거), keyword(fasttext_n_grams_오류제거)\n","\n","*키워드 병합*\n","-> keyword(3ver_결측값제거)\n","\n","*one len keyword 처리*\n","-> keyword(4ver_결측값제거), keyword(final)\n","\n","keyword(final): 타원 생성에 사용한 최종 데이터"],"metadata":{"id":"d2wFv_TL39kN"}},{"cell_type":"markdown","source":["## **1. 감성사전에서 키워드 추출**"],"metadata":{"id":"U8V8hBFAjH_S"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"QCai2W61dZdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import re\n","from konlpy.tag import Okt\n","\n","# KNU 한국어 감정 사전 로드\n","with open('원본 데이터/SentiWord_info.json', 'r', encoding='utf-8') as f:\n","    data = json.load(f)\n","\n","# CSV 파일 읽기\n","korean_data = pd.read_csv('원본 데이터/korean_data.csv')\n","\n","# 단어, 감정 강도 리스트\n","senti_dict = {entry['word']: abs(float(entry['polarity'])) for entry in data}\n","\n","# df 형태로 변환\n","senti_df = pd.DataFrame.from_dict(data=senti_dict, orient='index')\n","senti_df.reset_index(inplace=True)\n","senti_df.rename(columns={'index':'word', 0:'polarity'}, inplace=True)\n","\n","# 특수문자 제거\n","cleaned_korean_data = []\n","\n","for i in range(len(korean_data)):\n","  input_string = korean_data['Text'][i]\n","  cleaned_string = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", input_string)\n","  cleaned_korean_data.append(cleaned_string)\n","\n","# KoNLPy 형태소 분석기 초기화\n","okt = Okt()\n","\n","### 감정 스코어 계산 함수 ###\n","def calculate_emotion_score(word):\n","    score = 0.0\n","    if word in senti_dict:\n","        score = senti_dict[word]\n","    return score\n","\n","### n-gram 생성 ###\n","def generate_ngrams(sentence, n):\n","    \"\"\"\n","    주어진 문장을 n-gram으로 나누어서 리스트로 반환합니다.\n","\n","    :param sentence: 나눌 문장\n","    :param n: n-gram의 n 값 (1부터 n까지의 n-gram을 생성)\n","    :return: n-gram을 저장한 리스트\n","    \"\"\"\n","    ngrams = []\n","\n","    # 문장을 공백을 기준으로 단어로 분리\n","    words = sentence.split()\n","\n","    # n-gram 생성\n","    for i in range(len(words) - n + 1):\n","        ngram = ' '.join(words[i:i + n])\n","        ngrams.append(ngram)\n","\n","    return ngrams\n","\n","### 각 요소의 감정 스코어 계산 ###\n","def extract_emotional_keywords(sentence, num_keywords=5):\n","\n","    # 1-gram부터 n-gram까지 생성\n","    words = []\n","    words = words + okt.morphs(sentence)\n","\n","    for n in range(1, sentence.count(' ')+2):\n","      ngrams = generate_ngrams(sentence, n)\n","      words.extend(ngrams)\n","\n","    # 각 단어의 감정 스코어 계산\n","    emotion_scores = [(word, calculate_emotion_score(word)) for word in words]\n","\n","    # 스코어가 높은 순으로 정렬\n","    emotion_scores.sort(key=lambda x: x[1], reverse=True)\n","\n","    if not emotion_scores:\n","      keywords = \"\"\n","      return keywords\n","\n","    # 상위 num_keywords 개의 감정 키워드 추출\n","    keywords = [word for word, _ in emotion_scores[:num_keywords]]\n","\n","    return keywords\n","\n","### n-gram이 사전안에 있다면, keyword 추출 ###\n","korean_data['emotion_keyword'] = \"\"\n","\n","for i in range(len(korean_data)):\n","  sentence = cleaned_korean_data[i]\n","\n","  # 감정 키워드 추출\n","  emotion_keywords = extract_emotional_keywords(sentence)\n","\n","  if emotion_keywords:\n","      #print(\"감정 키워드를 찾았습니다:\")\n","      #print(emotion_keywords)\n","      korean_data['emotion_keyword'][i] = emotion_keywords[0]\n","  #else:\n","      #print(\"감정 키워드를 찾지 못했습니다.\")"],"metadata":{"id":"FVD7YC-JDVDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["korean_data"],"metadata":{"id":"m6M2bpW0lKbL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# korean_data.to_csv('키워드 데이터/keyword(감성사전).csv', index=False)"],"metadata":{"id":"8RvQ_SkenNyu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. fasttext 사용하여 키워드 추출**"],"metadata":{"id":"6a0HY-HxjWoC"}},{"cell_type":"markdown","source":["cc.ko.300.bin.gz는 해당 fasttext 사이트에서 Korean bin 파일 다운 받아 사용\n","\n","https://fasttext.cc/docs/en/crawl-vectors.html"],"metadata":{"id":"vT5rKhdbkCPd"}},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"IMKDTcesCYEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gunzip cc.ko.300.bin.gz"],"metadata":{"id":"A-TVpnH9LiP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"A3liTeC7pkpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from konlpy.tag import Okt\n","from gensim import models\n","\n","# CSV 파일 읽기\n","korean_data = pd.read_csv('원본 데이터/korean_data.csv')\n","\n","ko_model = models.fasttext.load_facebook_model('cc.ko.300.bin', encoding='utf-8')\n","\n","# 특수문자 제거\n","cleaned_korean_data = []\n","\n","for i in range(len(korean_data)):\n","  input_string = korean_data['Text'][i]\n","  cleaned_string = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", input_string)\n","  cleaned_korean_data.append(cleaned_string)\n","\n","# KoNLPy 형태소 분석기 초기화\n","okt = Okt()\n","\n","### n-gram 생성 (1-gram: n = 1, 2-grams: n = 2) ###\n","def generate_ngrams(sentence, n):\n","    \"\"\"\n","    주어진 문장을 n-gram으로 나누어서 리스트로 반환합니다.\n","\n","    :param sentence: 나눌 문장\n","    :param n: n-gram의 n 값 (1부터 n까지의 n-gram을 생성)\n","    :return: n-gram을 저장한 리스트\n","    \"\"\"\n","    ngrams = []\n","\n","    # 문장을 공백을 기준으로 단어로 분리\n","    words = sentence.split()\n","\n","    # n-gram 생성\n","    for i in range(len(words) - n + 1):\n","        ngram = ' '.join(words[i:i + n])\n","        ngrams.append(ngram)\n","\n","    return ngrams\n","\n","### 각 요소의 감정 스코어 계산 ###\n","def extract_emotional_keywords(sentence, target_emotion, num_keywords=5):\n","\n","    # 1-gram부터 n-gram까지 생성\n","    words = []\n","    words = words + okt.morphs(sentence)\n","\n","    #for n in range(1, sentence.count(' ')+2):\n","    for n in range(1, sentence.count(' ')+2):\n","      ngrams = generate_ngrams(sentence, n)\n","      words.extend(ngrams)\n","\n","    # 가장 유사한 단어를 찾습니다.\n","    most_similar_word = None\n","    max_similarity = -1  # 유사성 점수 초기화\n","\n","    for word in words:\n","      similarity = ko_model.wv.similarity(target_emotion, word)\n","      if similarity > max_similarity:\n","        max_similarity = similarity\n","        most_similar_word = word\n","\n","    return most_similar_word\n","\n","### keyword 추출 ###\n","korean_data['emotion_keyword'] = \"\"\n","\n","for i in range(len(korean_data)):\n","  sentence = cleaned_korean_data[i]\n","  target_emotion = korean_data['Emotion'][i]\n","\n","  # 감정 키워드 추출\n","  most_similar_word = extract_emotional_keywords(sentence, target_emotion)\n","\n","  if most_similar_word:\n","      #print(\"감정 키워드를 찾았습니다:\")\n","      #print(emotion_keywords)\n","      korean_data['emotion_keyword'][i] = most_similar_word\n","  #else:\n","      #print(\"감정 키워드를 찾지 못했습니다.\")"],"metadata":{"id":"ttuEwicapddc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["korean_data"],"metadata":{"id":"HxwaW2fnq5Tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# korean_data.to_csv('키워드 데이터/keyword(fasttext_1_gram).csv', index=False)\n","# korean_data.to_csv('키워드 데이터/keyword(fasttext_2_grams).csv', index=False)\n","# korean_data.to_csv('키워드 데이터/keyword(fasttext_n_grams).csv', index=False)"],"metadata":{"id":"TRjpEe6nvnCi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. fasttext 인코딩 오류 제거**\n"],"metadata":{"id":"428_BzzHsnKo"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# 키워드 데이터 전부 가져오기\n","gram1_data = pd.read_csv('키워드 데이터/keyword(fasttext_1_gram).csv')\n","gram2_data = pd.read_csv('키워드 데이터/keyword(fasttext_2_grams).csv')\n","gramn_data = pd.read_csv('키워드 데이터/keyword(fasttext_n_grams).csv')\n","dict_data = pd.read_csv('키워드 데이터/keyword(감성사전).csv')\n","\n","# gram1_data.isnull().sum()\n","# gram2_data.isnull().sum()\n","# gramn_data.isnull().sum()\n","# dict_data.isnull().sum()\n","\n","# gram1_data[gram1_data['emotion_keyword'].isnull()]\n","\n","# str 형태로 변경\n","gram1_data['emotion_keyword'] = gram1_data['emotion_keyword'].astype(str)\n","gram2_data['emotion_keyword'] = gram2_data['emotion_keyword'].astype(str)\n","gramn_data['emotion_keyword'] = gramn_data['emotion_keyword'].astype(str)\n","dict_data['emotion_keyword'] = dict_data['emotion_keyword'].astype(str)\n","\n","# gram1_data[gram1_data['emotion_keyword'] == \"\\xa0\"]['Emotion'].value_counts()\n","# gram2_data[gram2_data['emotion_keyword'] == \"\\xa0\"]['Emotion'].value_counts()\n","# gramn_data[gramn_data['emotion_keyword'] == \"\\xa0\"]['Emotion'].value_counts()\n","\n","# gram1_data[gram1_data['emotion_keyword'] == \"\\xa0\"]\n","# gram2_data[gram2_data['emotion_keyword'] == \"\\xa0\"]\n","\n","\n","for i in range(len(gram1_data)):\n","  if gram1_data['emotion_keyword'][i] == \"\\xa0\":\n","    gram1_data['emotion_keyword'][i] = gram2_data['emotion_keyword'][i]\n","\n","# gram1_data[gram1_data['emotion_keyword'] == \"\\xa0\"]\n","\n","for i in range(len(gram1_data)):\n","  if gram1_data['emotion_keyword'][i] == \"\\xa0\":\n","    gram1_data['emotion_keyword'][i] = dict_data['emotion_keyword'][i]\n","\n","for i in range(len(gram2_data)):\n","  if gram2_data['emotion_keyword'][i] == \"\\xa0\":\n","    gram2_data['emotion_keyword'][i] = dict_data['emotion_keyword'][i]\n","\n","for i in range(len(gramn_data)):\n","  if gramn_data['emotion_keyword'][i] == \"\\xa0\":\n","    gramn_data['emotion_keyword'][i] = dict_data['emotion_keyword'][i]\n","\n","# gramn_data[gramn_data['emotion_keyword'] == \"\\xa0\"]"],"metadata":{"id":"6V0fW_NHhRhx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gram1_data.to_csv(\"키워드 데이터/keyword(fasttext_1_gram_오류제거).csv\", index = False)\n","# gram2_data.to_csv(\"키워드 데이터/keyword(fasttext_2_gram_오류제거).csv\", index = False)\n","# gramn_data.to_csv(\"키워드 데이터/keyword(fasttext_n_grams_오류제거).csv\", index = False)"],"metadata":{"id":"ZLmjj0od4yl2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**4. 키워드 병합**"],"metadata":{"id":"AHcP2EqzvFzz"}},{"cell_type":"code","source":["import pandas as pd\n","\n","gram1_data = pd.read_csv('키워드 데이터/keyword(fasttext_1_gram_오류제거).csv')\n","gram2_data = pd.read_csv('키워드 데이터/keyword(fasttext_2_grams_오류제거).csv')\n","gramn_data = pd.read_csv('키워드 데이터/keyword(fasttext_n_grams_오류제거).csv')\n","dict_data = pd.read_csv('키워드 데이터/keyword(감성사전).csv')\n","\n","dict_data.rename(columns = {'emotion_keyword' : 'Keyword(dict)'}, inplace = True)\n","gram1_data.rename(columns = {'emotion_keyword' : 'Keyword(gram1)'}, inplace = True)\n","gram2_data.rename(columns = {'emotion_keyword' : 'Keyword(gram2)'}, inplace = True)\n","\n","final_df = pd.concat([dict_data,gram1_data['Keyword(gram1)']],axis=1, join='inner')\n","final_df = pd.concat([final_df,gram2_data['Keyword(gram2)']],axis=1, join='inner')\n","final_df.dropna(inplace=True)\n","final_df.reset_index(drop=True)\n","final_df"],"metadata":{"id":"iighqc5155Q2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# final_df.to_csv(\"키워드 데이터/keyword(3ver_결측값제거).csv\", index = False)"],"metadata":{"id":"RklqTcrZ64FN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**5. one len keyword 처리**\n"],"metadata":{"id":"nX-tyOSXw-LB"}},{"cell_type":"code","source":["import pandas as pd\n","\n","keyword_df = pd.read_csv('키워드 데이터/keyword(3ver_결측값제거).csv')\n","\n","keyword_df['Keyword(dict)'] = keyword_df['Keyword(dict)'].astype(str)\n","keyword_df['Keyword(gram1)'] = keyword_df['Keyword(gram1)'].astype(str)\n","keyword_df['Keyword(gram2)'] = keyword_df['Keyword(gram2)'].astype(str)"],"metadata":{"id":"sS8MYEDi7O3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_len_dict = {}\n","\n","for i in range(len(keyword_df)):\n","  if len(keyword_df['Keyword(dict)'][i]) == 1:\n","    one_len_dict[i] = keyword_df['Keyword(dict)'][i]\n","\n","one_len_gram1 = {}\n","\n","for i in range(len(keyword_df)):\n","  if len(keyword_df['Keyword(gram1)'][i]) == 1:\n","    one_len_gram1[i] = keyword_df['Keyword(gram1)'][i]\n","\n","one_len_gram2 = {}\n","\n","for i in range(len(keyword_df)):\n","  if len(keyword_df['Keyword(gram2)'][i]) == 1:\n","    one_len_gram2[i] = keyword_df['Keyword(gram2)'][i]\n","\n","print(len(one_len_dict))\n","print(len(one_len_gram1))\n","print(len(one_len_gram2))"],"metadata":{"id":"j1coWJZw7uNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추출하려는 인덱스 번호 리스트\n","indexes_to_extract = list(one_len_dict.keys())  # 추출하려는 인덱스 번호 목록\n","\n","# 인덱스 번호에 해당하는 값을 추출하여 새로운 DataFrame 생성\n","keyword_one_len_dict = keyword_df[keyword_df['Keyword(dict)'].isin([one_len_dict.get(idx) for idx in indexes_to_extract])]\n","\n","# 결과 출력\n","keyword_one_len_dict"],"metadata":{"id":"XWIdJHusBG0v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추출하려는 인덱스 번호 리스트\n","indexes_to_extract = list(one_len_gram1.keys())  # 추출하려는 인덱스 번호 목록\n","\n","# 인덱스 번호에 해당하는 값을 추출하여 새로운 DataFrame 생성\n","keyword_one_len_gram1 = keyword_df[keyword_df['Keyword(gram1)'].isin([one_len_gram1.get(idx) for idx in indexes_to_extract])]\n","\n","# 결과 출력\n","keyword_one_len_gram1"],"metadata":{"id":"7N-kDXnFCq9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 추출하려는 인덱스 번호 리스트\n","indexes_to_extract = list(one_len_gram2.keys())  # 추출하려는 인덱스 번호 목록\n","\n","# 인덱스 번호에 해당하는 값을 추출하여 새로운 DataFrame 생성\n","keyword_one_len_gram2 = keyword_df[keyword_df['Keyword(gram2)'].isin([one_len_gram2.get(idx) for idx in indexes_to_extract])]\n","\n","# 결과 출력\n","keyword_one_len_gram2"],"metadata":{"id":"qPvXuwGjDFYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keyword_df['Keyword(final)'] = \"\"\n","\n","for i in range(len(keyword_df)):\n","  if len(keyword_df['Keyword(dict)'][i]) == 1:\n","    if len(keyword_df['Keyword(gram1)'][i]) != 1:\n","      keyword_df['Keyword(final)'][i] = keyword_df['Keyword(gram1)'][i]\n","    elif len(keyword_df['Keyword(gram2)'][i]) != 1:\n","      keyword_df['Keyword(final)'][i] = keyword_df['Keyword(gram2)'][i]\n","    else:\n","      keyword_df['Keyword(final)'][i] = keyword_df['Keyword(dict)'][i]\n","  else:\n","    keyword_df['Keyword(final)'][i] = keyword_df['Keyword(dict)'][i]\n","\n","keyword_df"],"metadata":{"id":"tTPnYP3IEtYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# keyword_df.to_csv(\"키워드 데이터/keyword(4ver_결측값제거).csv\", index = False)\n","\n","new_keyword_df = keyword_df[['Text', 'Emotion', 'Arousal', 'Valence', 'Keyword(final)']]\n","# new_keyword_df.to_csv(\"키워드 데이터/keyword(final).csv\", index = False)"],"metadata":{"id":"Ocwks5llM_Ys"},"execution_count":null,"outputs":[]}]}